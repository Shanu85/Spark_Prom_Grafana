x-spark-common: &spark-common
  image: bitnami/spark:3.5.0
  volumes:
    - ./jobs:/opt/bitnami/spark/jobs
    - ./mnt/checkpoints:/mnt/spark-checkpoints
    - ./mnt/spark-state:/mnt/spark-state
  networks:
    - commonNetwork

x-kafka-controller-base: &kafka-controller-base
  image: apache/kafka:3.8.1 # supports KRaft mode, where Kafka runs without the need for Zookeeper.
  restart: on-failure
  command:
    - /bin/sh
    - -c
    - |
      echo '
      log4j.rootLogger=INFO, stdout, kafkaAppender 
      log4j.appender.stdout=org.apache.log4j.ConsoleAppender 
      log4j.appender.stdout.layout=org.apache.log4j.PatternLayout 
      log4j.appender.stdout.layout.ConversionPattern=[%d] %p %m (%c)%n 
      log4j.appender.kafkaAppender=org.apache.log4j.RollingFileAppender 
      log4j.appender.kafkaAppender.File=/opt/kafka/logs/kafka.log 
      log4j.appender.kafkaAppender.MaxFileSize=10MB 
      log4j.appender.kafkaAppender.MaxBackupIndex=10 
      log4j.appender.kafkaAppender.layout=org.apache.log4j.PatternLayout 
      log4j.appender.kafkaAppender.layout.ConversionPattern=[%d] %p %m (%c)%n
      ' > /tmp/log4j.properties 
      /etc/kafka/docker/run
  volumes:
    - ./volumes/jmx_exporter:/usr/share/jmx_exporter
  networks:
    - commonNetwork

x-kafka-controller-env: &kafka-controller-env
  KAFKA_PROCESS_ROLES: controller # it's a controller-only node, not a broker.
  KAFKA_LISTENERS: CONTROLLER://:9093
  KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
  KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
  KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT, PLAINTEXT:PLAINTEXT, PLAINTEXT_HOST:PLAINTEXT
  KAFKA_GROUP_INITIAL-REBALANCE_DELAY_MS: 0
  KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-controller-1:9093,2@kafka-controller-2:9093,3@kafka-controller-3:9093
  # Defines the KRaft quorum voters. Format: nodeId@host:port. Since this is a single-node quorum, it votes for itself.
  KAFKA_CLUSTER_ID: ""
  KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties

services:
  kafka-controller-1:
    <<: *kafka-controller-base
    container_name: kafka-controller-1
    ports:
      - "9300:9300"   # JMX Exporter metrics port
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 1
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"

    networks:
      - commonNetwork
    volumes:
      - controller_data_1:/var/lib/kafka/data
      # Maps a named Docker volume (controller_data_1) to the Kafka data directory inside the container. This ensures that metadata persists even if the container is stopped or removed.
      - ./logs/controller-logs_1:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/

  kafka-controller-2:
    <<: *kafka-controller-base
    container_name: kafka-controller-2
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 2
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"

    networks:
      - commonNetwork
    volumes:
      - controller_data_2:/var/lib/kafka/data
      - ./logs/controller-logs_2:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/

  kafka-controller-3:
    <<: *kafka-controller-base
    container_name: kafka-controller-3
    environment:
      <<: *kafka-controller-env
      KAFKA_NODE_ID: 3
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"

    networks:
      - commonNetwork
    volumes:
      - controller_data_3:/var/lib/kafka/data
      - ./logs/controller-logs_3:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/

  kafka-broker-1:
    <<: *kafka-controller-base
    container_name: kafka-broker-1
    ports:
      - "29092:9092" # 29092 -> Used by host clients to connect to Kafka, 19092 -> for internal cluster traffic, 9092 -> for external (host) traffic
    environment:
      KAFKA_NODE_ID: 4
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: PLAINTEXT://:19092,PLAINTEXT_HOST://:9092  # inside docker use PLAINTEXT://:19092 and for host use 9092 (mapped to 29092)
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-1:19092,PLAINTEXT_HOST://localhost:29092 # how others should reach this server, for host jobs use 29092, inside docker use 19092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT # no encryption of message
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-controller-1:9093,2@kafka-controller-2:9093,3@kafka-controller-3:9093
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"
    depends_on:
      - kafka-controller-1
      - kafka-controller-2
      - kafka-controller-3
    networks:
      - commonNetwork
    volumes:
      - ./volumes/broker_data_1:/var/lib/kafka/data
      - ./logs/broker-logs_1:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/
    healthcheck:
      test: [ "CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list"]
      interval: 10s
      timeout: 5s
      retries: 3

  kafka-broker-2:
    <<: *kafka-controller-base
    container_name: kafka-broker-2
    ports:
      - "39092:9092"
    environment:
      KAFKA_NODE_ID: 5
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-2:19092,PLAINTEXT_HOST://localhost:39092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-controller-1:9093,2@kafka-controller-2:9093,3@kafka-controller-3:9093
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"
    depends_on:
      - kafka-controller-1
      - kafka-controller-2
      - kafka-controller-3
    networks:
      - commonNetwork
    volumes:
      - ./volumes/broker_data_2:/var/lib/kafka/data
      - ./logs/broker-logs_2:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/

    healthcheck:
      test: [ "CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list" ]
      interval: 10s
      timeout: 5s
      retries: 3

  kafka-broker-3:
    image: apache/kafka:3.8.1
    container_name: kafka-broker-3
    ports:
      - "49092:9092"
    environment:
      KAFKA_NODE_ID: 6
      KAFKA_PROCESS_ROLES: broker
      KAFKA_LISTENERS: PLAINTEXT://:19092,PLAINTEXT_HOST://:9092
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://kafka-broker-3:19092,PLAINTEXT_HOST://localhost:49092
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_CONTROLLER_LISTENER_NAMES: CONTROLLER
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: CONTROLLER:PLAINTEXT,PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_CONTROLLER_QUORUM_VOTERS: 1@kafka-controller-1:9093,2@kafka-controller-2:9093,3@kafka-controller-3:9093
      KAFKA_LOG4J_OPTS: -Dlog4j.configuration=file:/tmp/log4j.properties
      KAFKA_OPTS: "-javaagent:/usr/share/jmx_exporter/jmx_prometheus_javaagent-1.1.0.jar=9300:/usr/share/jmx_exporter/kafka-broker.yml"
    depends_on:
      - kafka-controller-1
      - kafka-controller-2
      - kafka-controller-3
    networks:
      - commonNetwork
    volumes:
      - ./volumes/broker_data_3:/var/lib/kafka/data
      - ./logs/broker-logs_3:/opt/kafka/logs
      - ./volumes/jmx_exporter:/usr/share/jmx_exporter/

    healthcheck:
      test: [ "CMD", "kafka-topics", "--bootstrap-server", "localhost:9092", "--list" ]
      interval: 10s
      timeout: 5s
      retries: 3

  schema-registry:
    image: confluentinc/cp-schema-registry:7.5.1
    container_name: schema-registry
    ports:
      - "18081:8081"
    environment:
      SCHEMA_REGISTRY_HOST_NAME: schema-registry
      SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS: kafka-broker-1:19102,kafka-broker-2:19092,kafka-broker-3:19092
      SCHEMA_REGISTRY_KAFKASTORE_TOPIC: _schemas
      SCHEMA_REGISTRY_LISTENERS: http://0.0.0.0:18081
    depends_on:
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
    networks:
      - commonNetwork

  console:
    image: docker.redpanda.com/redpandadata/console:v2.5.2
    command: >-
      -c 'echo "$$CONSOLE_CONFIG_FILE" > /tmp/config.yml; /app/console'
    entrypoint: /bin/sh
    ports:
      - "8080:8080"
    depends_on:
      - schema-registry
    environment:
      CONFIG_FILEPATH: /tmp/config.yml
      CONSOLE_CONFIG_FILE: >
        kafka:
          brokers: ["kafka-broker-1:19092", "kafka-broker-2:19092", "kafka-broker-3:19092"]
          schemaRegistry:
            enabled: true
            urls: ["http://schema-registry:18081"]

        connect:
          enabled: false
          clusters:
            - name: local-connect-cluster
              url: http://connect:8083

    networks:
      - commonNetwork

  spark-master:
    <<: *spark-common
    container_name: spark-master
    command: bin/spark-class org.apache.spark.deploy.master.Master # Starts the Spark Master node
    ports:
      - "9190:8080"
      - "7077:7077"
      - "4040:4040"
      - "8081:8081"

  spark-worker: &spark-worker-image
    <<: *spark-common
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://spark-master:7077
    depends_on:
      - spark-master
    environment:
      SPARK_MODE: worker
      SPARK_WORKER_CORES: 2
      SPARK_WORKER_MEMORY: 2g
      SPARK_MASTER_URL: spark://spark-master:7077

  spark-worker-2:
    container_name: spark-worker-2
    <<: *spark-worker-image

  spark-worker-3:
    container_name: spark-worker-3
    <<: *spark-worker-image

  # for monitoring purpose
  prometheus:
    image: prom/prometheus:v3.0.0
    container_name: prometheus
    ports:
      - "9090:9090"
    depends_on:
      - kafka-broker-1
      - kafka-broker-2
      - kafka-broker-3
    volumes:
      - ./monitoring/prometheus:/etc/prometheus
    networks:
      - commonNetwork

  alertmanager:
    image: prom/alertmanager:v0.27.0
    ports:
      - "59093:9093"
    networks:
      - commonNetwork

  grafana:
    image: grafana/grafana:latest
    container_name: grafana
    ports:
      - "3000:3000"
    volumes:
      - grafana-data:/var/lib/grafana
      - ./monitoring/grafana/provisioning:/etc/grafana/provisioning
      - ./monitoring/grafana/dashboards:/var/lib/grafana/dashboards
    networks:
      - commonNetwork

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.1
    container_name: es-container
    environment:
      # Disable security for a simple setup
      - xpack.security.enabled=false
      - xpack.security.http.ssl.enabled=false
      - discovery.type=single-node
    ports:
      - "9200:9200"
    networks:
      - commonNetwork
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9200/_cluster/health" ]
      interval: 10s
      timeout: 10s
      retries: 5

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.1
    container_name: kb-container
    environment:
        # Connect Kibana to Elasticsearch using the service name
      - ELASTICSEARCH_HOSTS=http://es-container:9200
    depends_on:
      elasticsearch:
        condition: service_healthy
    ports:
      - "5601:5601"
    networks:
      - commonNetwork

  filebeat:
    image: docker.elastic.co/beats/filebeat:8.12.2
    user: root
    restart: unless-stopped
    volumes:
      - ./monitoring/elk/filebeat/filebeat.yml:/usr/share/filebeat/filebeat.yml:ro
      - ./logs/controller-logs_1:/opt/kafka/controller-logs_1:ro # ro means filebeat can only read logs nothing more
      - ./logs/controller-logs_2:/opt/kafka/controller-logs_2:ro # ro ensures that log files arenâ€™t accidentally altered by Filebeat
      - ./logs/controller-logs_3:/opt/kafka/controller-logs_3:ro
      - ./logs/broker-logs_1:/opt/kafka/broker-logs_1:ro
      - ./logs/broker-logs_2:/opt/kafka/broker-logs_2:ro
      - ./logs/broker-logs_3:/opt/kafka/broker-logs_3:ro
    environment:
      - ELASTICSEARCH_HOST=http://es-container:9200
    networks:
      - commonNetwork
    depends_on:
      - elasticsearch

networks:
  commonNetwork:

volumes:
  controller_data_1:
  controller_data_2:
  controller_data_3:
  broker_data_1:
  broker_data_2:
  broker_data_3:
  grafana-data:
